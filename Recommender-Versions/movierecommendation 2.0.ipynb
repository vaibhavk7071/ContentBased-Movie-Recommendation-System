{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-20T10:37:10.722480Z","iopub.execute_input":"2023-09-20T10:37:10.722833Z","iopub.status.idle":"2023-09-20T10:37:11.034810Z","shell.execute_reply.started":"2023-09-20T10:37:10.722808Z","shell.execute_reply":"2023-09-20T10:37:11.033234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom surprise import Reader, Dataset, SVD\nfrom surprise.model_selection import cross_validate\nfrom functools import reduce\n\nimport warnings; warnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T10:37:11.036538Z","iopub.execute_input":"2023-09-20T10:37:11.036940Z","iopub.status.idle":"2023-09-20T10:37:12.488831Z","shell.execute_reply.started":"2023-09-20T10:37:11.036915Z","shell.execute_reply":"2023-09-20T10:37:12.487711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"md = pd.read_csv('../input/the-movies-dataset/movies_metadata.csv')\nmd['genres'] = md['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n\nvote_counts = md[md['vote_count'].notnull()]['vote_count'].astype('int')\nvote_averages = md[md['vote_average'].notnull()]['vote_average'].astype('int')\nC = vote_averages.mean()\n\nm = vote_counts.quantile(0.95)\n\nmd['year'] = pd.to_datetime(md['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T10:37:12.490026Z","iopub.execute_input":"2023-09-20T10:37:12.490286Z","iopub.status.idle":"2023-09-20T10:37:15.190864Z","shell.execute_reply.started":"2023-09-20T10:37:12.490263Z","shell.execute_reply":"2023-09-20T10:37:15.189694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_recommendations_for_multiple_movies(movie_list, top_n=4):\n    with open('CosineVal.pkl', 'rb') as file:\n        cosine_sim = pickle.load(file)\n    \n    recommended_movies = []\n    \n    for movie_title in movie_list:\n        idx = indices.get(movie_title, None)\n        if idx is not None:\n            if(cosine_sim[idx].ndim > 1):\n                sim_scores = list(enumerate(cosine_sim[idx][0]))\n            else:\n                sim_scores = list(enumerate(cosine_sim[idx]))\n            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n            sim_scores = sim_scores[1:(top_n+1)]\n            movie_indices = [i[0] for i in sim_scores]\n            recommended_movies.extend(list(titles.iloc[movie_indices]))\n    return reduce(lambda re, x: re+[x] if x not in re else re, recommended_movies, [])","metadata":{"execution":{"iopub.status.busy":"2023-09-20T11:41:27.327955Z","iopub.execute_input":"2023-09-20T11:41:27.328348Z","iopub.status.idle":"2023-09-20T11:41:27.335843Z","shell.execute_reply.started":"2023-09-20T11:41:27.328317Z","shell.execute_reply":"2023-09-20T11:41:27.334660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credits = pd.read_csv('../input/the-movies-dataset/credits.csv')\nkeywords = pd.read_csv('../input/the-movies-dataset/keywords.csv')\n\nlinks_small = pd.read_csv('../input/the-movies-dataset/links_small.csv')\nlinks_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')\n\nkeywords['id'] = keywords['id'].astype('int')\ncredits['id'] = credits['id'].astype('int')\n\nmd = md.drop([19730, 29503, 35587])\n\nmd['id'] = md['id'].astype('int')\n\nmd = md.merge(credits, on='id')\nmd = md.merge(keywords, on='id')\n\nsmd = md[md['id'].isin(links_small)]\nsmd['cast'] = smd['cast'].apply(literal_eval)\nsmd['crew'] = smd['crew'].apply(literal_eval)\nsmd['keywords'] = smd['keywords'].apply(literal_eval)\nsmd['cast_size'] = smd['cast'].apply(lambda x: len(x))\nsmd['crew_size'] = smd['crew'].apply(lambda x: len(x))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T10:37:15.201988Z","iopub.execute_input":"2023-09-20T10:37:15.202263Z","iopub.status.idle":"2023-09-20T10:37:33.157483Z","shell.execute_reply.started":"2023-09-20T10:37:15.202240Z","shell.execute_reply":"2023-09-20T10:37:33.156254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","metadata":{"execution":{"iopub.status.busy":"2023-09-20T10:37:33.158593Z","iopub.execute_input":"2023-09-20T10:37:33.158893Z","iopub.status.idle":"2023-09-20T10:37:33.163708Z","shell.execute_reply.started":"2023-09-20T10:37:33.158868Z","shell.execute_reply":"2023-09-20T10:37:33.163015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smd['director'] = smd['crew'].apply(get_director)\n\nsmd['cast'] = smd['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\nsmd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)\n\nsmd['keywords'] = smd['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n\nsmd['cast'] = smd['cast'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])\nsmd['director'] = smd['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\nsmd['director'] = smd['director'].apply(lambda x: [x,x, x])\n\ns = smd.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'keyword'\ns = s.value_counts()\ns = s[s > 1]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T10:37:33.164700Z","iopub.execute_input":"2023-09-20T10:37:33.165449Z","iopub.status.idle":"2023-09-20T10:37:36.447488Z","shell.execute_reply.started":"2023-09-20T10:37:33.165425Z","shell.execute_reply":"2023-09-20T10:37:36.446229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filter_keywords(x):\n    words = []\n    for i in x:\n        if i in s:\n            words.append(i)\n    return words","metadata":{"execution":{"iopub.status.busy":"2023-09-20T10:37:36.448895Z","iopub.execute_input":"2023-09-20T10:37:36.449203Z","iopub.status.idle":"2023-09-20T10:37:36.454348Z","shell.execute_reply.started":"2023-09-20T10:37:36.449179Z","shell.execute_reply":"2023-09-20T10:37:36.453089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_rating(x):\n    v = x['vote_count']\n    R = x['vote_average']\n    return (v/(v+m) * R) + (m/(m+v) * C)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T10:37:36.455411Z","iopub.execute_input":"2023-09-20T10:37:36.455781Z","iopub.status.idle":"2023-09-20T10:37:36.472384Z","shell.execute_reply.started":"2023-09-20T10:37:36.455759Z","shell.execute_reply":"2023-09-20T10:37:36.471156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer = SnowballStemmer('english')\n\nsmd['keywords'] = smd['keywords'].apply(filter_keywords)\nsmd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])\nsmd['keywords'] = smd['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])\n\nsmd['soup'] = smd['keywords'] + smd['cast'] + smd['director'] + smd['genres']\nsmd['soup'] = smd['soup'].apply(lambda x: ' '.join(x))\n\ncount = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ncount_matrix = count.fit_transform(smd['soup'])\n\ncosine_sim = cosine_similarity(count_matrix, count_matrix)\n\nsmd = smd.reset_index()\ntitles = smd['title']\nindices = pd.Series(smd.index, index=smd['title'])","metadata":{"execution":{"iopub.status.busy":"2023-09-20T10:37:36.475101Z","iopub.execute_input":"2023-09-20T10:37:36.475387Z","iopub.status.idle":"2023-09-20T10:37:39.594109Z","shell.execute_reply.started":"2023-09-20T10:37:36.475364Z","shell.execute_reply":"2023-09-20T10:37:39.592703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('CosineVal.pkl', 'wb') as file:\n      \n    # A new file will be created\n    pickle.dump(cosine_sim, file)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T11:37:39.933760Z","iopub.execute_input":"2023-09-20T11:37:39.934133Z","iopub.status.idle":"2023-09-20T11:37:40.646230Z","shell.execute_reply.started":"2023-09-20T11:37:39.934102Z","shell.execute_reply":"2023-09-20T11:37:40.645034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_recommendations_for_multiple_movies(['3 Idiots'], 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T11:41:32.326722Z","iopub.execute_input":"2023-09-20T11:41:32.327093Z","iopub.status.idle":"2023-09-20T11:41:33.081042Z","shell.execute_reply.started":"2023-09-20T11:41:32.327061Z","shell.execute_reply":"2023-09-20T11:41:33.080112Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
